# Q2 - Decision Trees

The data given is as follows - 

| Attr1 | Attr2 | Attr3 | Attr4 | Attr5 | Class |
| :---: | :---: | :---: | :---: | :---: | :---: |
|   3   | BLUE  | SMALL |  LOW  | COOL  |   F   |
|   7   | BLUE  | LARGE | HIGH  | COOL  |   F   |
|  16   | BLUE  | LARGE |  LOW  | COOL  |   F   |
|  17   | BLUE  | LARGE | HIGH  | COOL  |   F   |
|  27   | BLUE  | LARGE | HIGH  |  HOT  |   T   |
|  29   | BLUE  | SMALL | HIGH  |  HOT  |   T   |
|  33   | BLUE  | SMALL |  LOW  |  HOT  |   F   |
|  34   | BLUE  | LARGE | HIGH  |  HOT  |   T   |
|   2   |  RED  | LARGE |  LOW  | COOL  |   F   |
|   6   |  RED  | SMALL |  LOW  | COOL  |   T   |
|  10   |  RED  | SMALL | HIGH  | COOL  |   T   |
|  11   |  RED  | SMALL |  LOW  | COOL  |   F   |
|  25   |  RED  | SMALL | HIGH  |  HOT  |   T   |
|  36   |  RED  | LARGE | HIGH  |  HOT  |   T   |
|  45   |  RED  | SMALL |  LOW  |  HOT  |   F   |
|  50   |  RED  | LARGE |  LOW  |  HOT  |   F   |

The entropy of Class label is (9F and 7T)- 

$H(Class) = -[\frac{9}{16}\log_2(\frac{9}{16}) + \frac{7}{16}\log_2(\frac{7}{16})] = $