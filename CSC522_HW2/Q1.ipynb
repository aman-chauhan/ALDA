{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q1 - PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import the libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (A) Load the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_and_headers(filename):\n",
    "    data = None\n",
    "    with open(filename) as fp:\n",
    "        data = [x.strip().split(',') for x in fp.readlines()]\n",
    "    headers = data[0]\n",
    "    headers = np.asarray(headers)\n",
    "    class_field = len(headers) - 1\n",
    "    data_x = [[float(x[i]) for i in range(class_field)] for x in data[1:]]\n",
    "    data_x = np.asarray(data_x)\n",
    "    data_y = [[str(x[i]) for i in range(class_field, class_field + 1)] for x in data[1:]]\n",
    "    data_y = np.asarray(data_y)\n",
    "    return headers, data_x, data_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "headers, train_x, train_y = data_and_headers('Data' + os.sep + 'hw2q1_train.csv')\n",
    "headers, test_x, test_y = data_and_headers('Data' + os.sep + 'hw2q1_test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Training Data')\n",
    "print('Number of features - ' + str(train_x.shape[1]))\n",
    "print('Number of target features - ' + str(train_y.shape[1]))\n",
    "print('Number of observations - ' + str(train_x.shape[0]))\n",
    "print('Number of observations in category R - ' + str(train_y[train_y=='R'].shape[0]))\n",
    "print('Number of observations in category M - ' + str(train_y[train_y=='M'].shape[0]))\n",
    "print()\n",
    "print('Testing Data')\n",
    "print('Number of features - ' + str(test_x.shape[1]))\n",
    "print('Number of target features - ' + str(test_y.shape[1]))\n",
    "print('Number of observations - ' + str(test_x.shape[0]))\n",
    "print('Number of observations in category R - ' + str(test_y[test_y=='R'].shape[0]))\n",
    "print('Number of observations in category M - ' + str(test_y[test_y=='M'].shape[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (B) Normalization and PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(data, minima, maxima):\n",
    "    normal = np.copy(data)\n",
    "    normal = (normal - minima) / (maxima - minima)\n",
    "    return normal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normal_train = normalize(train_x, np.amin(train_x, axis=0), np.amax(train_x, axis=0))\n",
    "normal_test = normalize(test_x, np.amin(train_x, axis=0), np.amax(train_x, axis=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (i) Covariance of Training Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "covariance = np.cov(normal_train, rowvar=False)\n",
    "fig, axes = plt.subplots(nrows=1, ncols=1)\n",
    "fig.set_figheight(12)\n",
    "fig.set_figwidth(15)\n",
    "im = axes.pcolor(covariance, cmap='CMRmap')\n",
    "fig.colorbar(im, ax=axes)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (ii) Eigenvalue and Eigenvectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Size of covariance matrix - ' + str(covariance.shape))\n",
    "w,v = np.linalg.eig(covariance)\n",
    "print('Top 5 Eigenvalues - ' + ', '.join(['{:.3f}'.format(x) for x in w[:5]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (iii) Plot of Eigenvalues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(nrows=1, ncols=3)\n",
    "fig.set_figheight(6)\n",
    "fig.set_figwidth(24)\n",
    "axes[0].bar(np.arange(60), w)\n",
    "axes[0].xaxis.grid()\n",
    "axes[0].yaxis.grid()\n",
    "axes[0].set_xlabel('eigenvectors')\n",
    "axes[0].set_ylabel('eigenvalues')\n",
    "axes[0].set_title('Plot of Variance captured by each eigenvector')\n",
    "axes[1].bar(np.arange(60), np.cumsum(w))\n",
    "axes[1].xaxis.grid()\n",
    "axes[1].yaxis.grid()\n",
    "axes[1].set_xlabel('eigenvectors')\n",
    "axes[1].set_ylabel('cumulative sum of eigenvalues')\n",
    "axes[1].set_title('Cumulative Contribution of eigenvalues')\n",
    "axes[2].plot(np.arange(60), np.cumsum(w)/np.sum(w))\n",
    "axes[2].xaxis.grid()\n",
    "axes[2].yaxis.grid()\n",
    "axes[2].set_xlabel('eigenvectors')\n",
    "axes[2].set_ylabel('cumulative sum of eigenvalues (scaled)')\n",
    "axes[2].set_title('Scaled Cumulative Contribution of eigenvalues')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Number of Eigenvectors - 10\n",
    "***This is because if we look at the scaled cumulative contribution of the eigen values, we can see that the first 10 eigenvalues cover almost 80% of all variance in the data set. So 10 seems to be a good choice for selecting the number of eigenvectors.***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (iv) PCA with KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ncomps = [2,4,6,8,10,20,40,60]\n",
    "cls = [KNeighborsClassifier(n_neighbors=3, metric='euclidean').fit(np.matmul(normal_train, v[:,:ncomps[i]]), np.ravel(train_y)) for i in range(len(ncomps))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred4 = cls[4].predict(np.matmul(normal_test, v[:,:ncomps[4]]))\n",
    "true4 = np.ravel(test_y)\n",
    "with open('q1iv.csv','w') as fp:\n",
    "    fp.write(', '.join(['Component' + str(i) for i in range(1,ncomps[4]+1)])+', Class, Class\\n')\n",
    "    transformed = np.matmul(normal_test, v[:,:ncomps[4]])\n",
    "    for i in range(len(transformed)):\n",
    "        fp.write(', '.join([str(x) for x in transformed[i]]))\n",
    "        fp.write(', ' + str(true4[i]))\n",
    "        fp.write(', ' + str(pred4[i]) + '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***You can find the output of the above code in file q1iv.csv***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(nrows=1, ncols=1)\n",
    "fig.set_figheight(6)\n",
    "fig.set_figwidth(8)\n",
    "accuracies = [cls[i].score(np.matmul(normal_test, v[:,:ncomps[i]]), np.ravel(test_y)) for i in range(len(ncomps))]\n",
    "axes.bar(ncomps, accuracies)\n",
    "axes.set_xlabel('Number of Components')\n",
    "axes.set_ylabel('Accuracy of Prediction')\n",
    "axes.set_title('Performance of Classification for all components')\n",
    "plt.grid()\n",
    "plt.show()\n",
    "print('Accuracy - ')\n",
    "print('Components\\tAccuracy')\n",
    "for i in range(len(ncomps)):\n",
    "    print('{}\\t\\t{:.4f}%'.format(ncomps[i],100*accuracies[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reasonable Number of PCA components - 10\n",
    "***The reasons are 2-fold - 1) We predicted it above when we plotted the eigenvalues and 2) It gives the best accuracy out of the given number of components.***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (C) Standardization and PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def standardize(data, mean, sd):\n",
    "    standard = np.copy(data)\n",
    "    standard = (standard - mean) / sd\n",
    "    return standard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "standard_train = standardize(train_x, np.mean(train_x, axis=0), np.std(train_x, axis=0))\n",
    "standard_test = standardize(test_x, np.mean(train_x, axis=0), np.std(train_x, axis=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (i) Covariance of Training Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "covariance = np.cov(standard_train, rowvar=False)\n",
    "fig, axes = plt.subplots(nrows=1, ncols=1)\n",
    "fig.set_figheight(12)\n",
    "fig.set_figwidth(15)\n",
    "im = axes.pcolor(covariance, cmap='CMRmap')\n",
    "fig.colorbar(im, ax=axes)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (ii) Eigenvalue and Eigenvectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Size of covariance matrix - ' + str(covariance.shape))\n",
    "w,v = np.linalg.eig(covariance)\n",
    "print('Top 5 Eigenvalues - ' + ', '.join(['{:.3f}'.format(x) for x in w[:5]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (iii) Plot of Eigenvalues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(nrows=1, ncols=3)\n",
    "fig.set_figheight(6)\n",
    "fig.set_figwidth(24)\n",
    "axes[0].bar(np.arange(60), w)\n",
    "axes[0].xaxis.grid()\n",
    "axes[0].yaxis.grid()\n",
    "axes[0].set_xlabel('eigenvectors')\n",
    "axes[0].set_ylabel('eigenvalues')\n",
    "axes[0].set_title('Plot of Variance captured by each eigenvector')\n",
    "axes[1].bar(np.arange(60), np.cumsum(w))\n",
    "axes[1].xaxis.grid()\n",
    "axes[1].yaxis.grid()\n",
    "axes[1].set_xlabel('eigenvectors')\n",
    "axes[1].set_ylabel('cumulative sum of eigenvalues')\n",
    "axes[1].set_title('Cumulative Contribution of eigenvalues')\n",
    "axes[2].plot(np.arange(60), np.cumsum(w)/np.sum(w))\n",
    "axes[2].xaxis.grid()\n",
    "axes[2].yaxis.grid()\n",
    "axes[2].set_xlabel('eigenvectors')\n",
    "axes[2].set_ylabel('cumulative sum of eigenvalues (scaled)')\n",
    "axes[2].set_title('Scaled Cumulative Contribution of eigenvalues')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Number of Eigenvectors - 20\n",
    "***This is because if we look at the scaled cumulative contribution of the eigen values, we can see that the first 20 eigenvalues cover almost 90% of all variance in the data set. Also, beyond that, all the eigenvectors combined can only contribute 10% more variance. So 20 seems to be a good choice for selecting the number of eigenvectors.***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (iv) PCA with KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ncomps = [2,4,6,8,10,20,40,60]\n",
    "cls = [KNeighborsClassifier(n_neighbors=3, metric='euclidean').fit(np.matmul(standard_train, v[:,:ncomps[i]]), np.ravel(train_y)) for i in range(len(ncomps))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred4 = cls[4].predict(np.matmul(standard_test, v[:,:ncomps[4]]))\n",
    "true4 = np.ravel(test_y)\n",
    "with open('q1v.csv','w') as fp:\n",
    "    fp.write(', '.join(['Component' + str(i) for i in range(1,ncomps[4]+1)])+', Class, Class\\n')\n",
    "    transformed = np.matmul(standard_test, v[:,:ncomps[4]])\n",
    "    for i in range(len(transformed)):\n",
    "        fp.write(', '.join([str(x) for x in transformed[i]]))\n",
    "        fp.write(', ' + str(true4[i]))\n",
    "        fp.write(', ' + str(pred4[i]) + '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***You can find the output of the above code in file q1v.csv***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(nrows=1, ncols=1)\n",
    "fig.set_figheight(6)\n",
    "fig.set_figwidth(8)\n",
    "accuracies = [cls[i].score(np.matmul(standard_test, v[:,:ncomps[i]]), np.ravel(test_y)) for i in range(len(ncomps))]\n",
    "axes.bar(ncomps, accuracies)\n",
    "axes.set_xlabel('Number of Components')\n",
    "axes.set_ylabel('Accuracy of Prediction')\n",
    "axes.set_title('Performance of Classification for all components')\n",
    "plt.grid()\n",
    "plt.show()\n",
    "print('Accuracy - ')\n",
    "print('Components\\tAccuracy')\n",
    "for i in range(len(ncomps)):\n",
    "    print('{}\\t\\t{:.4f}%'.format(ncomps[i],100*accuracies[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reasonable Number of PCA components - 20\n",
    "***The reason is simple - It gives the best accuracy out of the given number of components. If we look at the cumulative plot of eigen values above, we can see that 20 eigenvectors were able to cover almost 90% of the variance. As we keep increasing/decreasing the number of components from this point, the accuracy seems to fall gradually.***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (D) Preference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comparing both procedures, I believe that standardization is better than normalization for this dataset. Not only did PCA + KNN with standardized data give better accuracy, the eigenvalues were much smoother and more interpretable. We can clearly see how many attributes the top 10 eigen vectors were covering, giving a good intuition about the number of eigen vectors/number of PCA components to choose."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
